
### Word Embedding

이 내용을 기반으로 정리하면, 자연어 처리(NLP)에서의 데이터 전처리와 단어 표현 방법, 그리고 언어 모델의 필요성에 대해 아래와 같이 설명할 수 있다.

### **자연어 처리(NLP) 데이터 전처리**

단어를 표현하기 위한 이전 단계들은 다음과 같다.

EDA > Preprocessing > Tokenization > Token Encoding

1. **EDA(탐색적 데이터 분석)**: 자연어 코퍼스 분석으로 워드 클라우드, 시각화 툴 활용이 포함된다.
2. **전처리 과정**:
    - **노이즈 제거**: 불필요한 정보 제거.
    - **정규 표현식**: 패턴 기반의 텍스트 처리.
    - **노말라이제이션**: 텍스트 표준화.
    - **라벨링**: 비지도 학습에서 수동으로 라벨 부여.
    - **토큰화**: 형태소 분석, 의미 단위 분리, 맞춤법 교정 등을 포함.
    - **문장 단위 분리**: 센텐스 토크나이저 활용.
    - **토큰 임베딩 및 인덱스 매핑**: 모델 입력을 위한 준비.

### **단어 표현 방법**

자연어 처리에서는 텍스트의 특징 추출을 위해 수치화를 해주어야 한다. 수치화를 해주는 방법에는 두 가지 방법이 있다.

- **로컬 표현(Local Representation)**: 단어의 빈도에 초점을 맞춘 방식(Bag of Words, DTM, TF-IDF, BM25). 해당 단어 그 자체만 보고 특정값을 매핑하여 단어를 표현하는 방법.
- **연속 표현(Continuous Representation) / 분산 표현(Distributed Representation)**: 단어의 의미와 맥락을 고려한 벡터 표현(Word2Vec, GloVe 등). 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법

### **언어 모델의 필요성**

- **언어 모델**: 주어진 단어들의 시퀀스에서 다음 단어를 예측하고 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측 . 자연어 이해와 생성에 필수적.
- **SLM(Statistical Language Model)**: 단어열이 가지는 확률분호를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델. 모델의 목표는 실제로 많이 사용하는 단어열(문장)의 분포를 정확하게 근사하는데 목표를 둠

### N-gram 언어 모델

- 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부단어(n개)만 고려하는 접근 방법

discrete word representation

### **Bag of Words 모델**

- `강사님` “말 그대로 가방에 넣는 것. 순서를 고려하지 않음. 단지 출현 빈도, frequency에만 집중하는 데이터 수치화 표현 방법이다. 공백 토큰화를 해서 단어별로 가방에 집어넣는다. “
- Bag of Words(BoW) 모델은 텍스트 데이터를 수치화하는 가장 기본적인 방법 중 하나로, 문서나 텍스트에서 각 단어의 출현 빈도(frequency)만을 고려한다. 이 모델에서는 단어의 순서나 문맥은 고려되지 않으며, 오직 단어가 몇 번 등장했는지만이 중요하다. BoW 모델을 만드는 과정은 다음 두 가지 주요 단계로 이루어진다:
- 두 가지 단계
    1. **각 단어에 고유 정수 인덱스 부여한다**: 모든 고유 단어에 대해 고유한 정수 인덱스를 할당합니다. 이 과정을 통해 단어집합(Vocabulary)이 생성된다.
    2. **단어 토큰의 등장 횟수로 벡터 생성**: 각 문서에 대해, 단어집합에 있는 각 단어가 해당 문서에서 몇 번 등장하는지 세어서 벡터로 표현한다. 벡터의 각 요소는 단어집합에 있는 단어의 인덱스에 해당하며, 값은 그 단어가 문서에서 등장하는 횟수를 의미한다.

### Document term matrix (DTM) 문서 단어 행렬

- `강사님` ”bag of words의 확장 버전이라고 생각하면 됩니다. 다수 문서의 등장하는 각 단어들의 빈도를 행렬로 표현한 것. 각 문서에 대한 bow를 하나의 행렬로 만든 것”
- Document-Term Matrix(DTM)는 다수의 문서에 등장하는 각 단어의 빈도를 행렬로 표현한 것입니다. Bag of Words(BoW) 모델의 확장된 형태로 볼 수 있으며, 각 문서에 대한 BoW를 하나의 큰 행렬로 표현한 것이다. DTM을 생성하는 과정은 다음과 같다.
- DTM 생성 과정
    1. **공백 토큰화(Whitespace Tokenization)**: 모든 문서의 단어들을 공백을 기준으로 분리한다. 이렇게 분리된 단어들을 중복 없이 모아 단어집합(어휘 사전)을 만든다.
    2. **빈도수 기록**: 각 문서에서 어휘 사전에 있는 각 단어가 몇 번 나왔는지를 계산하여 BoW 벡터를 만든다. 이 벡터들을 모아서 하나의 큰 행렬로 구성하게 되며, 이것이 DTM이다. 각 문서는 행렬의 행으로, 각 단어는 열로 표현된다.

### DTM의 한계점

1. **희소 표현(Sparse Representation)**
    - 각 문서 벡터의 값으로 one-hot vector와 마찬가지로 절대 다수의 값이 0이 된다.
    - 많은 문서 벡터가 대부분의 공간을 0 값으로 채우게 된다. (예: 희소벡터, 희소행렬).
    - 따라서 희소벡터는 많은 양의 저장공간과 계산 리소스가 필요하게 된다.
2. **단순 빈도 수 기반 접근**
    - 모든 단어가 같은 중요도를 가진다는 가정 하에 단순한 빈도수에만 집중한다.
    - 이는 예를 들어 `'the'`와 같은 불용어가 빈번히 등장함에도 불구하고 실제로는 중요하지 않음에도 높은 중요도를 갖게 하는 문제점을 야기한다.
    - 그런데, 유사한 문맥에서 비교하고 싶은 문서1, 문서2, 문서3에서 동일하게 `'the'`가 반복되기 보다는 해당 이 문서들이 유사한 문서들이라고 판단하는 것은 적절하지 않는다.

**→ DTM 의 한계점을 해결 하기 위해 나온 것이 TF-IDF**

TF-IDF는 각 단어의 중요도를 문서 내에서의 빈도수와 문서 집합에서의 분포도를 동시에 고려하여 계산한다. 이 방법은 단어의 빈도는 높지만 문서 간에 고르게 분포되어 중요하지 않은 단어(예: 'the')의 가중치를 줄여줌으로써, 각 단어가 해당 문서에서 가지는 실질적인 중요도를 보다 정확히 반영한다.

### **TF-IDF (Term Frequency-Inverse Document Frequency)**

TF-IDF는 특정 단어가 문서 내에서 얼마나 중요한지를 수치화하는 방법.

TF-IDF는 이 두 값(TF와 IDF)을 곱한 값으로, 문서 내에서의 단어의 중요도를 나타내는 가중치로 활용된다. TF-IDF 값이 높을수록 문서 내에서 중요한 단어로 간주된다.

- TF(Term Frequency):
    - **TF (Term Frequency)**: 문서 _d_ 내의 단어 _t_가 등장하는 횟수를 문서 내의 총 단어 수로 나눈 값.
    - 문서 내에서의 특정 단어의 빈도수를 의미한다. 이 값은 해당 단어가 문서 내에서 얼마나 자주 등장하는지를 나타낸다. 하지만 'the', '는' 등의 빈도수가 높지만 정보를 많이 제공하지 않는 단어의 영향을 줄이기 위해 TF만 사용하지 않는다.
- IDF(Inverse Document Frequency):
    - **IDF (Inverse Document Frequency)**: 단어 _t_가 출현한 문서의 수를 역수로 취하고 로그를 취한 값. 이는 문서 집합 내에서 단어 _t_의 희귀성을 반영한다.
    - 단어가 드문 정도를 나타내는 수치이다. 특정 단어가 많은 문서에 등장하면 IDF 값은 낮아지고, 적은 문서에만 등장한다면 IDF 값은 높아진다. 이는 단어가 가진 희소성을 반영한다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/ad27c0a2-5e52-4768-8973-ed2233f47770/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/a3a9eb2a-0caa-4147-b1f3-4749b84b9345/Untitled.png)

### **벡터 유사도**

벡터 유사도 측정은 두 벡터 간의 유사성을 정량적으로 평가하는 방법입니다. 여기서 사용하는 주요 방법들은:

- **코사인 유사도 (Cosine Similarity)**: 두 벡터 간의 코사인 각도를 계산하여 유사성을 평가합니다. 값이 1에 가까울수록 유사도가 높습니다.
- **유클리드 거리 (Euclidean Distance)**: 두 벡터 간의 직선 거리를 계산합니다.
- **자카드 유사도 (Jaccard Similarity)**: 두 집합 간의 교집합을 합집합으로 나눈 값으로 유사도를 평가합니다.

---

# 07 RNN

### **시계열 데이터란?**

시계열 데이터는 **시간** 순서대로 발생하는 데이터 포인트들의 연속입니다. 예를 들어, 주식 가격, 날씨 데이터, 심박수 모니터링 등이 시계열 데이터의 예입니다. 이러한 데이터는 시간의 흐름에 따라 변화하며, 이전의 데이터 포인트들이 후속 데이터 포인트들에 영향을 미칠 수 있습니다.

### **RNN 구조**

RNN(Recurrent Neural Network, 순환 신경망)은 시계열 데이터나 자연어 처리와 같이 데이터의 순서가 중요한 문제를 다루기 위해 설계된 신경망 구조입니다. RNN은 내부에 반복되는 네트워크 모듈을 가지며, 이 모듈은 각 시점에서의 입력과 이전 시점의 상태를 고려하여 현재 시점의 상태를 계산합니다. 이를 통해 RNN은 시간적인 정보를 내부 상태에 저장하며, 이 정보를 이용해 시계열 데이터의 패턴을 학습할 수 있습니다.

### **다양한 RNN 구조들**

RNN은 다양한 형태로 변형되어 특정 작업에 적합한 구조를 가질 수 있습니다.

- **One to One**: 전통적인 신경망 구조와 같이, 하나의 입력이 들어와 하나의 출력을 내는 구조입니다. 시계열 데이터와는 관련이 적습니다.
- **One to Many**: 하나의 입력으로부터 시퀀스를 생성하는 구조입니다. 예를 들어, 이미지에 대한 설명을 생성하는 이미지 캡셔닝 작업에 사용될 수 있습니다.
- **Many to One**: 입력 시퀀스로부터 하나의 출력을 생성하는 구조입니다. 감정 분석이나 스팸 메일 분류 같은 작업에 사용됩니다.
- **Many to Many**: 입력 시퀀스와 출력 시퀀스의 길이가 같은 경우와 다른 경우 두 가지 형태가 있습니다. 전자는 시계열 데이터를 다른 시계열 데이터로 변환하는 작업에, 후자는 자연어 처리에서 문장 번역 같은 작업에 사용됩니다.

### **RNN의 한계점**

RNN은 장기 의존성 문제(long-term dependencies)를 가집니다. 이는 RNN이 시퀀스가 길어질수록 시퀀스의 초기 부분에서의 정보를 끝까지 전달하는 데 어려움을 겪는다는 것을 의미합니다. 이로 인해 모델이 중요한 정보를 잊어버리게 되어 성능이 저하될 수 있습니다.

### **LSTM, GRU**

장기 의존성 문제를 해결하기 위해 개발된 구조가 LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)입니다. 이들은 RNN의 기본 구조에 게이트라는 메커니즘을 추가하여 정보를 보다 효율적으로 저장하고 전달할 수 있게 합니다. LSTM은 세 가지 게이트(입력 게이트, 삭제 게이트, 출력 게이트)를 사용하고, GRU는 두 가지 게이트(업데이트 게이트, 리셋 게이트)를 사용합니다. 이 구조들은 RNN보다 복잡하지만, 장기 의존성 문제에 대해 보다 나은 성능을 보입니다.

### **정보의 표현**

- 정보는 미리 정해진 "고정된" 사이즈의 벡터로 표현할 수 있습니다. 이는 데이터를 신경망에 입력하기 위해 필요한 단계로, 모든 입력 데이터는 네트워크가 처리할 수 있는 고정된 크기의 벡터로 변환되어야 합니다. 예를 들어, 문장을 단어 또는 문자의 시퀀스로 분해하고, 각 단어나 문자를 벡터로 인코딩하여 신경망에 공급할 수 있습니다.

### **시계열 데이터**

- 시계열 데이터는 시간 순서대로 배열된 데이터의 집합입니다. 이러한 데이터는 음성 인식, 자연어 처리, 센서 데이터 분석, 주가 예측 등 다양한 분야에서 중요하게 사용됩니다. 시계열 데이터는 시간의 흐름에 따라 그 특성이 변화하므로, 이러한 변화를 포착하여 기록 추세를 분석하거나, 실시간으로 경고를 생성하거나, 미래의 상태를 예측하는 모델을 구축하는 데 사용될 수 있습니다.

### **시계열 데이터와 RNN**

- 시계열 데이터를 처리하기 위해 RNN(Recurrent Neural Network)이 자주 사용됩니다. RNN은 시간적 순서를 가진 데이터를 처리하는 데 최적화된 신경망 구조로, 이전의 정보를 기억하면서 새로운 입력과 함께 현재의 출력을 결정합니다. 하지만 시계열 데이터를 처리하는 데 RNN만이 유일한 해결책은 아닙니다.
- 그럼에도 불구하고 RNN을 사용하는 이유 :
    - MLP나 CNN은 입력데이터를 처음 한 번만 입력 받고 이후의 연산과정에서 지난 데이터를 추가적으로 고려할 일이 없다.
    - RNN은 입력의 연산 결과가 현재 입력데이터와 함께 고려된다.
    - RNN은 사앹를 계산할 때는 이전상태(state)를 사용하게 된다.

### **대안적인 접근 방법**

- 시계열 데이터에 대한 Classification(분류) 또는 Regression(회귀) 문제를 해결할 때, Multi-layer Perceptron(MLP)이나 Convolutional Neural Network(CNN)과 같은 다른 신경망 구조를 사용할 수도 있습니다. MLP는 여러 층의 퍼셉트론으로 구성되어 복잡한 패턴을 학습할 수 있으며, CNN은 이미지 처리뿐만 아니라 시계열 데이터를 효율적으로 처리할 수 있는 능력을 가지고 있습니다. 특히, 시계열 데이터에 대한 CNN의 사용은 시퀀스 내에서 지역적인 패턴을 효과적으로 포착할 수 있다는 점에서 유리합니다.

이처럼 RNN, MLP, CNN 등 다양한 신경망 구조를 시계열 데이터의 특성과 문제의 성격에 맞춰 적절히 선택하고 활용하는 것이 중요합니다. 이를 통해 보다 정확하고 효율적인 데이터 분석과 모델링이 가능해집니다.

시퀀스(문장)의 길이가 서로 다를 때, 특히 머신러닝이나 딥러닝 모델을 학습시키기 위해서는 모든 입력 시퀀스의 길이를 동일하게 맞춰주는 **전처리** 작업이 필요합니다. 길이가 짧은 문장을 처리하는 몇 가지 방법은 다음과 같습니다:

1. **패딩(Padding) 추가**: 가장 일반적인 방법은 길이가 짧은 문장에 패딩을 추가하여 모든 문장의 길이를 최대 길이의 문장에 맞추는 것입니다. 일반적으로 패딩은 문장의 시작(Pre-padding)이나 끝(Post-padding)에 특정 값(예: 0)을 삽입하여 수행됩니다.
2. **문장 자르기(Truncating)**: 반대로, 문장의 길이가 너무 길 경우, 특정 길이로 자르는 방법이 있습니다. 이는 주로 길이가 긴 문장을 다룰 때 사용되며, 문장의 중요하지 않은 부분을 제거할 수 있습니다.
3. **문장 병합**: 여러 짧은 문장을 하나의 긴 문장으로 병합하는 방법도 있습니다. 이 경우, 문맥이나 내용의 흐름을 방해하지 않도록 주의해야 합니다.
4. **특별 토큰 사용**: 특정 작업에 따라, 문장의 시작이나 끝을 나타내는 특별 토큰(**`<s>`**, **`</s>`**, **`<pad>`** 등)을 사용할 수 있습니다. 이 방법은 모델이 문장의 시작과 끝을 인식하는 데 도움이 될 수 있습니다.
5. **데이터 증강(Data Augmentation)**: 길이가 짧은 문장에 추가적인 정보를 더하거나 문장을 재구성하는 방법으로, 데이터를 인위적으로 증가시키는 방법입니다. 예를 들어, 동의어 교체, 문장의 일부를 재배치하는 등의 방법이 있습니다. 이는 모델이 더 다양한 데이터에 대해 학습할 수 있게 하여 성능을 향상시킬 수 있습니다.

어떤 방법을 선택할지는 주로 작업의 목적, 사용되는 모델, 그리고 데이터의 특성에 따라 달라질 수 있습니다. 패딩은 가장 기본적이고 널리 사용되는 방법이며, 대부분의 신경망 라이브러리에서 쉽게 구현할 수 있는 기능을 제공합니다.

RNN(Recurrent Neural Network)의 기본 구조는 순차적 데이터 처리를 위해 설계되었습니다. 이 구조는 시간에 따른 정보의 흐름을 모델링할 수 있는 능력을 가지고 있으며, 이전의 정보를 현재의 결정에 반영할 수 있도록 만들어져 있습니다. RNN의 핵심은 네트워크 내에서의 반복적인 연산 루프(loop)입니다. 이 루프를 통해 네트워크는 정보를 시간에 걸쳐 전달할 수 있으며, 이전 시점의 출력이 다음 시점의 입력으로 활용됩니다.

### **RNN의 주요 구성 요소**

1. **입력층 (Input Layer)**: 시계열 데이터 또는 순차적 데이터의 각 요소가 네트워크로 입력됩니다. 이 입력은 주로 벡터 형태로 되어 있으며, 시간의 흐름에 따라 차례대로 네트워크에 제공됩니다.
2. **은닉층 (Hidden Layer)**: RNN의 핵심 부분으로, 이전 시점의 은닉 상태와 현재 시점의 입력을 결합하여 현재 시점의 은닉 상태를 생성합니다. 이 과정에서 네트워크는 가중치(W)를 사용하여 입력 데이터와 이전 은닉 상태의 정보를 통합하고, 비선형 활성화 함수(예: tanh, ReLU)를 통과시켜 새로운 은닉 상태를 계산합니다. 이 은닉 상태는 시간에 따른 정보를 내재하고 있으며, 네트워크가 과거의 데이터로부터 학습한 정보를 저장합니다.
3. **출력층 (Output Layer)**: 은닉층에서 계산된 은닉 상태는 출력층으로 전달되며, 여기에서 최종 출력이 생성됩니다. 출력은 특정 작업(분류, 회귀, 시퀀스 생성 등)에 맞춰서 설계될 수 있으며, 은닉 상태를 바탕으로 다음 시점의 데이터를 예측하거나, 현재 시점의 데이터에 대한 분류 결정을 내릴 수 있습니다.

## 다양한 RNN 구조들

---

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/2a252ea8-7163-4583-9c76-71da6c2d2c27/Untitled.png)

### **RNN의 작동 원리**

RNN의 작동 원리는 매우 간단하면서도 강력합니다. 각 시점에서의 입력 ��_xt_와 이전 시점의 은닉 상태 ℎ�−1_ht_−1가 주어졌을 때, 네트워크는 다음과 같이 현재 시점의 은닉 상태 ℎ�_ht_를 계산합니다:

RNN의 작동 원리는 매우 간단하면서도 강력합니다. 각 시점에서의 입력 ��_xt_와 이전 시점의 은닉 상태 ℎ�−1_ht_−1가 주어졌을 때, 네트워크는 다음과 같이 현재 시점의 은닉 상태 ℎ�_ht_를 계산합니다:

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/6c24bb24-27b0-4c40-8d66-aadb4df63e29/Untitled.png)

h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)

여기서:

- **`h_t`**는 현재 시점의 은닉 상태입니다.
- **`f`**는 활성화 함수입니다 (예: tanh, ReLU).
- **`W_{hh}`**는 이전 은닉 상태에서 현재 은닉 상태로의 변환을 위한 가중치입니다.
- **`h_{t-1}`**는 이전 시점의 은닉 상태입니다.
- **`W_{xh}`**는 현재 입력에서 은닉 상태로의 변환을 위한 가중치입니다.
- **`x_t`**는 현재 시점의 입력입니다.
- **`b_h`**는 편향입니다.

이 과정을 통해, RNN은 시간에 따라 정보를 전달하면서, 각 시점에서의 은닉 상태를 업데이트합니다. 이 은닉 상태는 네트워크가 지니고 있는 "메모리"로 볼 수 있으며, 이 메모리를 통해 RNN은 과거의 정보를 현재의 결정에 반영할 수 있습니다.

RNN(Recurrent Neural Network, 순환신경망)은 시퀀스 데이터 처리에 적합한 인공 신경망의 한 유형입니다. RNN의 핵심 아이디어는 내부에 순환(loop) 구조를 가지고 있어 이전의 정보를 기억하면서 새로운 입력과 함께 처리할 수 있다는 점입니다. 이를 통해 시간적인 순서가 중요한 데이터, 예를 들어 텍스트나 시계열 데이터를 처리할 때 유용합니다.

RNN의 'one to one' 모델은 가장 기본적인 RNN 형태로, 각 시간 단계마다 정확히 하나의 입력을 받고 하나의 출력을 생성합니다. 이는 고정 크기의 입력을 받아 고정 크기의 출력을 내는 전통적인 신경망과 유사한 구조입니다. 그러나 RNN은 이러한 각 단계에서 이전 단계의 정보를 '은닉 상태(hidden state)'를 통해 전달받아 처리할 수 있습니다.

RNN의 기본 수식은 다음과 같습니다:

```python
h_t = f(W_hh * h_(t-1) + W_xh * x_t + b_h)
```

```python
y_t = g(W_hy * h_t + b_y)
```

### 여기서,

- **`h_t`**: 시간 t에서의 은닉 상태
- **`x_t`**: 시간 t에서의 입력
- **`y_t`**: 시간 t에서의 출력
- **`W_hh`**, **`W_xh`**, **`W_hy`**: 학습할 가중치 매트릭스
- **`b_h`**, **`b_y`**: 편향
- **`f`**, **`g`**: 비선형 활성화 함수 (예: 시그모이드, tanh)

### 설명:

이 수식들은 RNN이 입력을 받고, 이전 상태의 정보를 기억하여 새로운 출력을 생성하는 과정을 수학적으로 표현합니다. **`f`**와 **`g`**는 각각 은닉 상태와 출력을 위한 비선형 활성화 함수로, 네트워크의 비선형 특성을 도입해주며, 이를 통해 모델은 복잡한 데이터 패턴을 학습할 수 있습니다.

위의 형식을 사용하면 노션에 붙여넣을 때 깨지지 않고 잘 표시될 것입니다.

한편, 'one to one' RNN 모델을 도식화한 이미지를 생성해 보겠습니다. 이 이미지는 기본적인 RNN 구조를 나타낼 것이며, 입력, 은닉 상태, 출력이 어떻게 각 시간 단계에서 처리되는지 보여줄 것입니다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/5f6298dd-9d84-435a-be09-bba1fa60f328/Untitled.png)

이 수식들은 RNN이 입력을 받고, 이전 상태의 정보를 기억하여 새로운 출력을 생성하는 과정을 수학적으로 표현합니다. **`f`**와 **`g`**는 각각 은닉 상태와 출력을 위한 비선형 활성화 함수로, 네트워크의 비선형 특성을 도입해주며, 이를 통해 모델은 복잡한 데이터 패턴을 학습할 수 있습니다.

RNN의 'one to many' 모델은 단일 입력으로부터 시퀀스를 생성하는 구조로, 예를 들어 하나의 이미지를 입력받아 해당 이미지의 설명을 여러 단어로 출력하는 이미지 캡셔닝(Image Captioning) 작업에 자주 사용됩니다. 이 모델은 시작 시점에 단일 입력을 받고, 그 이후 여러 시간 단계에 걸쳐 연속적인 출력을 생성합니다.

## 다양한 RNN 구조들

### **RNN 'One to Many' 모델 기본 구조:**

- **입력 단계:** 단일 입력 _x_가 모델에 주어집니다. 이 입력은 초기 은닉 상태 ℎ0의 계산에 사용됩니다.
- **출력 단계:** 이후 각 시간 단계에서, 모델은 이전 은닉 상태를 기반으로 새로운 은닉 상태를 계산하고, 각 은닉 상태에 대해 하나의 출력 _yt_를 생성합니다.

### **기본 수식:**

1. **초기 은닉 상태 계산:**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/879c90f0-cf53-45bf-a2c3-fe9ef4803c58/Untitled.png)

1. **이후 은닉 상태 업데이트 (시간 단계 _t_에서):**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/19e0d3f0-97a4-42ad-8b0f-ec194d851b9f/Untitled.png)

1. **출력 (시간 단계 _t_에서):**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/98852900-83fa-4465-9de3-7362348ecdcb/Untitled.png)

여기서,

- _x_: 초기 입력
- _ht_: 시간 _t_에서의 은닉 상태
- _yt_: 시간 _t_에서의 출력
- _Wx_ℎ, _W_ℎℎ, _W_ℎ_y_: 학습할 가중치 매트릭스
- _b_ℎ, _by_: 편향
- _f_, _g_: 비선형 활성화 함수 (예: 시그모이드, tanh)

이제 'one to many' RNN 모델의 동작을 시각화한 이미지를 생성하여 설명을 더욱 쉽게 이해할 수 있도록 하겠습니다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/a7cb7d73-d0c9-4335-8397-c54cb6b3e143/Untitled.png)

위 이미지는 'one to many' RNN 모델을 시각화한 것입니다. 모델은 단일 입력(_x_)을 받아 초기 은닉 상태(ℎ0_h_0)를 계산하고, 이후 여러 시간 단계에 걸쳐 연속적인 출력(_yt_)을 생성합니다. 각 시간 단계에서 모델은 이전 은닉 상태를 사용하여 새로운 은닉 상태를 계산하고, 해당 은닉 상태를 기반으로 출력을 생성합니다. 이 과정에서 가중치 _Wx_ℎ, _W_ℎℎ, _W_ℎ_y_와 편향 _b_ℎ, _by_가 사용됩니다. 이러한 구조는 특히 시퀀스 데이터 생성 작업에 유용하며, 내부적으로 이전의 정보를 기억하면서 새로운 출력을 순차적으로 생성할 수 있습니다.

이 설명과 이미지를 노션에 붙여넣기 하시면 'one to many' RNN 모델의 개념과 동작 방식을 쉽게 이해하고 공유할 수 있을 것입니다.

RNN의 'many to one' 모델은 시퀀스 데이터를 입력받아 단일 출력을 생성하는 구조입니다. 이 모델은 주로 감성 분석, 텍스트 분류, 시계열 데이터의 예측과 같이 입력 시퀀스의 전체적인 정보를 요약해 하나의 결과를 내는 작업에 사용됩니다. 예를 들어, 여러 단어로 이루어진 문장이 주어졌을 때 그 문장의 전체적인 감성이 긍정적인지 부정적인지를 판단하는 경우가 여기에 해당합니다.

### **RNN 'Many to One' 모델 기본 구조:**

- **입력 단계:** 시퀀스의 각 요소가 차례로 모델에 입력됩니다.
- **은닉 상태 업데이트:** 각 입력마다 모델은 이전 은닉 상태를 업데이트하며, 시퀀스의 모든 정보를 마지막 은닉 상태에 반영하도록 합니다.
- **최종 출력 생성:** 시퀀스의 마지막 요소가 처리된 후, 모델은 최종 은닉 상태를 기반으로 단일 출력을 생성합니다.

### **기본 수식:**

1. **은닉 상태 업데이트 (시간 단계 _t_에서):**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/16f51c88-3fae-4576-b625-043682107104/Untitled.png)

1. **최종 출력:**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/d3be4964-978a-4c13-b023-22cbb9a498e3/Untitled.png)

여기서,

- _xt_: 시간 _t_에서의 입력
- ℎ_t_: 시간 _t_에서의 은닉 상태
- _y_: 최종 출력
- _Wx_ℎ, _W_ℎℎ, _W_ℎ_y_: 학습할 가중치 매트릭스
- _b_ℎ, _by_: 편향
- _f_, _g_: 비선형 활성화 함수 (예: 시그모이드, tanh)
- _T_: 입력 시퀀스의 마지막 시간 단계

이 모델은 시퀀스의 각 요소를 차례대로 처리하며, 최종적으로 시퀀스 전체의 정보를 요약해 단일 결과를 도출합니다.

### **이미지로 설명:**

이제 'many to one' RNN 모델의 동작을 시각화한 이미지를 생성하여, 이 모델이 어떻게 시퀀스 데이터를 처리하여 최종적으로 단일 출력을 생성하는지 설명을 더욱 쉽게 이해할 수 있도록 하겠습니다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/b53ad450-ba09-4769-84b3-67b0b907ded9/Untitled.png)

위 이미지는 'many to one' RNN 모델을 시각화한 것입니다. 이 모델은 시퀀스 데이터(여러 개의 입력 _xt_)를 순차적으로 받아들여, 각 시간 단계마다 은닉 상태(ℎ_t_)를 업데이트하며 전체 시퀀스의 정보를 종합합니다. 시퀀스의 마지막 입력이 처리된 후, 모델은 최종 은닉 상태를 기반으로 단일 출력(_y_)을 생성합니다. 이 과정에서 가중치 _Wx_ℎ, _W_ℎℎ, _W_ℎ_y_와 편향 _b_ℎ, _by_가 사용됩니다.

이 모델은 특히 텍스트 분류, 감성 분석, 시계열 데이터의 예측과 같이 전체 시퀀스 데이터를 하나의 결과로 요약해야 하는 작업에 적합합니다. 내부적으로 이전 입력들의 정보를 은닉 상태를 통해 계속 전달하며, 최종적으로 시퀀스 전체에 대한 결론을 내립니다.

이 설명과 이미지를 노션에 붙여넣기 하시면 'many to one' RNN 모델의 개념과 동작 방식을 쉽게 이해하고 공유할 수 있을 것입니다.

RNN의 'many to many' 모델은 시퀀스 데이터를 입력받아 다른 시퀀스 데이터를 출력하는 구조입니다. 이 모델은 두 가지 주요 형태로 나뉩니다: 입력 시퀀스와 출력 시퀀스의 길이가 동일한 경우와, 그렇지 않은 경우입니다. 첫 번째 형태는 예를 들어 시간 단계별로 비디오의 각 프레임을 분류하는 작업에 사용되며, 두 번째 형태는 기계 번역과 같이 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있는 작업에 사용됩니다.

### **RNN 'Many to Many' 모델 기본 구조 (동일 길이):**

- **입력 단계:** 시퀀스의 각 요소가 차례로 모델에 입력됩니다.
- **은닉 상태 업데이트:** 각 입력마다 모델은 이전 은닉 상태를 업데이트합니다.
- **출력 생성:** 각 시간 단계에서 모델은 현재 은닉 상태를 기반으로 출력을 생성합니다.

### **기본 수식:**

1. **은닉 상태 업데이트 (시간 단계 �_t_에서):**