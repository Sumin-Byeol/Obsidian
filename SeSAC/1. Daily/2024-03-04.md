## 미니 배치, 배치 크기, 이터레이션 및 파이토치 데이터 로드 방법 요약

### 1. 미니 배치(Mini Batch)와 배치 크기(Batch Size)
- **미니 배치:** 대규모 데이터셋을 효율적으로 학습하기 위하여 전체 데이터를 작은 단위로 나누어 학습하는 개념입니다. 각 작은 단위를 미니 배치라고 합니다.
- **배치 크기:** 각 미니 배치에 포함된 샘플의 수입니다. 배치 경사 하강법 대신 미니 배치 경사 하강법을 사용하여 훈련 속도를 높이고 메모리 사용량을 줄일 수 있습니다.
- **배치 경사 하강법 vs 미니 배치 경사 하강법:** 전체 데이터를 한 번에 사용하는 배치 경사 하강법은 계산량이 많고 메모리 사용량이 크지만, 안정적으로 최적값에 수렴합니다. 미니 배치 경사 하강법은 전체 데이터의 일부만을 보고 수행하여 훈련 속도가 빠르며 메모리 효율이 좋습니다.

### 2. 이터레이션(Iteration)
- 한 에포크 내에서 이루어지는 매개변수(가중치) 업데이트 횟수입니다. 전체 데이터를 미니 배치로 나누었을 때, 이터레이션의 수는 전체 데이터셋을 모두 처리하기 위해 필요한 미니 배치의 개수와 같습니다.

### 3. 데이터 로드하기
- **파이토치 도구:** 텐서 데이터셋(`TensorDataset`)과 데이터 로더(`DataLoader`)를 사용하여 미니 배치 학습, 데이터 셔플, 병렬 처리를 쉽게 수행할 수 있습니다.
- **TensorDataset:** 텐서를 입력받아 데이터셋 형태로 변환합니다.
- **DataLoader:** 데이터셋과 배치 크기 등의 인자를 입력받아 학습에 사용할 데이터 배치를 생성합니다. `shuffle=True` 옵션을 사용하면 에포크마다 데이터셋의 순서를 섞어 학습의 효율성을 높일 수 있습니다.

### MLP의 설계와 문제점

- **입력 차원:** MLP 설계 시 입력 차원은 데이터 변수의 개수에 따라 결정됩니다. 예를 들어, 데이터 변수가 3개라면 입력 차원은 (None, 3)이 됩니다. 여기서 'None'은 배치 크기를 나타내며, 이는 모델이 어떠한 배치 크기도 처리할 수 있음을 의미합니다.
- **문제점:**
    - **인풋 값의 민감도:** 입력 데이터가 조금만 변화해도 인풋 값이 크게 변화합니다.
    - **데이터 형상의 무시:** MLP는 이미지의 공간적인 정보를 무시하고 모든 입력을 독립적으로 처리합니다.
    - **가중치의 양:** 학습해야 할 가중치의 수가 많아짐에 따라 모델의 복잡성과 과적합의 위험이 증가합니다.

### CNN의 구성과 장점

- **구성 요소:**
    
    - **Convolution 층:** 선언된 크기의 필터(Filter)를 이미지에 적용해 특징을 추출합니다.
    - **Pooling 층:** 추출된 특징 중 중요한 부분을 강조하여 데이터의 크기를 줄이고, 불필요한 정보를 제거합니다. 대표적인 방법으로는 Max Pooling과 Average Pooling이 있습니다.
    - **Fully Connected (FC) 층:** 최종적으로 추출된 특징을 바탕으로 분류를 수행합니다.
- **커널의 학습 가능성:** CNN에서 사용되는 커널들은 데이터로부터 특징을 학습할 수 있어야 합니다.
    
- **채널 정보의 일치:** 이미지의 채널 정보와 필터의 채널 정보가 일치해야 합니다.
    
- **Activation Map 생성:** 필터를 통해 Feature Map에서 더 도드라지는 특징을 강조합니다.
    
- **장점:**
    
    - **End-to-End 모델:** CNN은 특징 추출과 분류를 한 번에 학습할 수 있는 end-to-end 모델을 제공합니다.
    - **성능:** CNN은 이미지 분류 작업에서 높은 성능을 보이며, 특히 딥러닝 기반의 이미지 분류에서 강점을 가집니다.
    - **데이터 요구량:** 높은 성능을 달성하기 위해서는 대량의 데이터가 필요합니다.

CNN은 이미지와 같은 고차원 데이터에서 MLP보다 더 나은 성능을 보이는 경향이 있습니다. 이는 CNN이 이미지의 공간적 계층 구조를 활용하고, 중요한 특징만을 학습하기 때문입니다. 반면, MLP는 모든 연결이 중요하며, 이는 데이터의 공간적 구조를 무시하는 결과를 낳습니다.

# MLP의 한계

MLP의 문제점은 이미지가 조금만 변해도 입력 값이 크게 달라진다는 것이다. MLP는 데이터의 공간적인 정보를 무시하고 학습해야 할 가중치가 많다는 단점이 있다.

**fully connected layers의 단점**

- **인풋 값의 민감도:** 입력 데이터가 조금만 변화해도 인풋 값이 크게 변화한다.
- **데이터 형상의 무시:** MLP는 이미지의 공간적인 정보를 무시하고 모든 입력을 독립적으로 처리한다.
- **가중치의 양:** 학습해야 할 가중치의 수가 많아짐에 따라 모델의 복잡성과 과적합의 위험이 증가한다.

이를 해결하기 위해 이미지의 특징을 추출하고 이러한 변화에 대응하는 방법으로 CNN이 개발되었다.

학습을 통해서 특징을 추출 ⇒ CNN

~~CNN은 이미지 특징을 추출하기 때문에 이미지를 1차원 배열로 펼쳐서 모델입력으로 넣지 않아도 된다. 2차원 배열 그대로 입력에 넣어주면 된다. CNN을 거친 후에 1차원 배열로 만들기는 해야한다..~~

# CNN

CNN은 이미지의 공간적 구조를 보존하고, 여러 필터를 통해 특징을 추출한다.

CNN(Convolutional Neural Network)은 이미지의 공간적인 정보를 효과적으로 활용하고, **변화에 강한 특징을 학습**하기 위해 개발된 신경망 구조다. CNN은 MLP의 한계를 대체한다. MLP는 이미지를 일렬로 펼쳐서 처리하는데, 이는 공간적인 정보를 잃게 만든다. 반면, CNN은 이미지를 1차원으로 펼치지 않아도 된다.

## CNN의 세 가지 층

CNN은 크게 세 가지 주요 층으로 구성된다:

- **Convolution 층** : 이 층에서는 다양한 크기의 필터를 이미지에 적용하여 특징을 추출한다. 각 필터는 이미지의 다른 특성을 감지하여 여러 피처맵을 생성한다.예를 들어, 세로 필터, 가로 필터, 대각선 필터 등을 사용하여 이미지의 세로선, 가로선, 대각선 등의 특징을 추출한다.
- **Pooling 층** : Convolution층을 통해 추출된 특징 맵(feature map)에서 중요한 정보만을 추려내는 과정이다. 이는 네트워크가 이미지의 작은 변화에 덜 민감하게 만들고 오버피팅을 줄이는 데 도움이 된다. 대표적으로 맥스 풀링(max pooling)과 에버리지 풀링(average pooling)이 있다.
- **Fully Connected(FC) 층 :** 앞서 추출된 특징들을 바탕으로 최종적으로 분류를 수행한다. 이 층은 일반적으로 신경망의 마지막 부분에 위치하며, 클래스에 속할 확률을 출력하는 데 사용된다.

### Convolution Layer

**Convolution layer Process**

- 입력 데이터의 처리: 컨볼루션 레이어에 입력되는 이미지는 여러 채널(RGB의 3채널 등)을 가진 2차원 데이터로, 각 채널은 별도의 2차원 매트릭스로 표현된다.
- 필터(커널)의 적용: 사전에 정의된 가중치를 가진 작은 2차원 매트릭스인 필터가 이미지 위를 이동하며 적용된다. 이 필터들은 이미지의 특정 특징을 감지하기 위해 사용된다.
- 요소별 곱셈과 합산: 필터는 이미지의 각 위치에서 해당 부분과 요소별 곱을 수행한 후 결과를 합산하여 피처맵의 하나의 값으로 저장한다.
- 스트라이드의 적용: 필터가 이미지를 스캔하는 동안 스트라이드(stride)를 사용하여 한 번에 이동하는 픽셀의 수를 결정한다. 스트라이드가 1이면, 필터는 한 번에 한 픽셀씩 이동하며 전체 이미지를 스캔한다.
- 피처맵의 생성: 필터가 이미지 전체에 적용되면서 생성된 결과를 피처맵에 순차적으로 저장한다. 이 피처맵은 입력 이미지에서 중요한 특징을 나타낸다.
- 비선형 활성화 함수: 컨볼루션 연산 후에는 피처맵에 비선형 활성화 함수를 적용하여 모델이 비선형 문제를 해결할 수 있도록 한다. 가장 일반적으로 사용되는 활성화 함수는 ReLU이다.

convolution layer에서 28_28⇒26_26 로 사이즈가 바뀌는 이유

**padding(패딩)**

- **패딩을 사용하는 이유 :**

이미지를 입력으로 받아 필터를 적용하면 활성화 맵(Activation map)이 생성된다. 이 과정을 반복하면서 새로운 필터를 적용하면서 Activation map을 계속 생성하게 된다. 이 과정을 거칠수록 Activation map의 크기는 점차적으로 줄어든다. 이러한 현상 때문에, 네트워크의 레이어를 깊게 쌓으면서 분류를 위한 필터 맵의 수가 적어진다. 이런 문제를 해결하기 위해 패딩(Padding)을 적용한다.

- **패딩 사용 방법 및 장점**

Padding은 필터를 적용하기 전에 입력 데이터 주변에 0으로 채워넣는 과정이다. 패딩은 filter를 override하기 전에 입력데이터에 padding을 적용해서 activation map의 크기를 유지시켜준다. 이렇게 함으로써 입력과 특성 맵의 크기를 동일하게 유지할 수 있고, 이미지의 가장자리에 위치한 픽셀 정보의 손실을 방지하며, 패딩으로 채워진 값은 실제로는 0이므로 계산에 영향을 미치지 않는다.

### Pooling Layer

**Pooling Layer 사용 이유**

- 데이터의 차원을 축소하여 표현을 간결하고 관리하기 쉽게 만든다.
- 각각의 activation map에 독립적으로 적용되어 효율적인 처리를 가능하게 한다.

**맥스(Max) 풀링:**

receptive field 내에서 가장 큰 값을 선택해 새로운 feature map을 만드는 과정이다.

- 각 리셉티브 필드(피처맵의 지역적인 영역)에서 가장 큰 값을 선택하여 다음 레이어로 전달한다.
- 일반적으로 2x2 크기의 풀링 필터를 사용하고, 이 경우 스트라이드도 2로 설정하여 겹치지 않게 한다.
- 결과적으로 피처맵의 크기는 가로와 세로 각각 절반으로 줄어든다.

```python
**Max Pooling를 쓰는 이유 :** 
- Convolution layer를 통해 추출된 feature map에서 중요한 정보를 강조하고 데이터의 크기를 줄임으로써 overfitting을 감소시킨다.
- Pooling layer는 parameter가 없으므로, network의 복잡도를 줄이고 overfitting 위험을 낮춘다.
- parameter가 없기 때문에 계산량과 하드웨어 자원(에너지)을 절약하며, 처리 속도를 향상시킨다.
- stride(스트라이드)를 사용하여 오버랩 없이 특정 간격으로 pooling을 수행하여 feature map을 축소한다.

Max Pooling 특징
- 파라미터가 없다. 연산할 것이 없어 빠르다. 가로와 세로의 크기가 반으로 줄어든다. 

```

**에버리지(Average) 풀링:**

receptive field 내의 값들의 평균을 계산하여 새로운 feature map을 만드는 과정이다.

- receptive field 내의 값들의 평균을 계산하여 평균값을 다음 레이어로 전달한다.
- 에버리지 풀링은 영역 전체의 평균적인 특징을 다음 레이어로 전달하는 효과를 가진다..

---

### **Fully Connected(FC) 층**