### **언어 모델의 중요성**

언어 모델(Language Model)은 자연어 처리(Natural Language Processing, NLP)의 핵심 요소 중 하나입니다. 그 중요성은 텍스트 데이터의 구조와 의미를 컴퓨터가 이해하고 생성할 수 있도록 하는 데 있습니다. 예를 들어, 기계 번역, 음성 인식, 문장 생성 등 다양한 NLP 응용 분야에서 언어 모델이 활용됩니다.

### **텍스트 표현하기: Embedding**

텍스트를 컴퓨터가 처리할 수 있는 형태로 변환하는 일은 NLP에서 중요한 단계입니다. 이를 위해 'Embedding'이라는 기술이 사용됩니다. Embedding은 단어나 문장과 같은 텍스트를 고정된 길이의 벡터로 변환하는 과정입니다. 이 벡터는 컴퓨터가 이해하고 처리할 수 있는 형태로, 단어의 의미, 문맥, 문법적 특성 등을 수치적으로 표현합니다.

### **대표적인 Embedding 모델들**

1. **Word2Vec**: Google에 의해 개발된 모델로, 주변 단어의 문맥을 기반으로 단어의 벡터를 학습합니다. 'CBOW(Continuous Bag of Words)'와 'Skip-Gram' 두 가지 방식으로 훈련될 수 있습니다.
2. **GloVe(Global Vectors for Word Representation)**: 스탠포드 대학에서 개발된 모델로, 전체 코퍼스의 단어 공기(co-occurrence) 통계를 이용하여 벡터를 학습합니다. Word2Vec과 달리, 전체 코퍼스의 정보를 한번에 학습합니다.
3. **FastText**: Facebook AI Research에 의해 개발된 모델로, Word2Vec을 기반으로 하지만, 단어를 더 작은 단위인 n-gram의 조합으로 보고 이를 벡터로 표현합니다. 이는 희귀 단어나 오타가 있는 단어의 표현에 강점을 가집니다.

### **언어모델의 필요성과 동계적 언어 모델(SLM)**

언어 모델의 궁극적 목표는 자연어로 이루어진 데이터의 확률 분포를 모델링하는 것입니다. 이를 통해 문장이나 문서의 자연스러움을 평가하거나, 다음에 올 단어를 예측하는 등의 작업을 수행할 수 있습니다.

동계적 언어 모델(Statistical Language Model, SLM)은 이러한 작업을 확률적으로 접근합니다. 단어의 시퀀스에 확률을 할당함으로써, 어떤 단어의 시퀀스가 더 자연스러운지를 판단할 수 있습니다. 하지만 이 모델은 충분한 크기와 다양성을 가진 말뭉치를 필요로 하며, 그렇지 않을 경우 자연어의 복잡성을 제대로 모델링하지 못하는 한계를 가집니다.

## N-gram 언어 모델

자연어 처리 분야에서 기본적이면서도 중요한 모델 중 하나입니다. 'N-gram'이란 텍스트에서 연속적으로 등장하는 N개의 항목(주로 단어)을 의미합니다. 이 모델은 특정 단어 시퀀스의 등장 확률을 계산함으로써, 문장이나 문서의 자연스러움을 평가하고, 다음 단어를 예측하는 데 사용됩니다.

### **Uni-gram (1-gram)**

Uni-gram 모델에서는 텍스트를 하나의 단어로만 이루어진 'gram'으로 분석합니다. 각 단어의 등장 확률은 해당 단어의 빈도수를 전체 단어 수로 나눈 것으로 계산됩니다. 이 모델은 단어의 문맥이나 순서를 고려하지 않기 때문에 가장 단순한 형태의 언어 모델입니다.

### **Bi-gram (2-gram)**

Bi-gram 모델은 텍스트를 두 단어의 쌍으로 분석합니다. 이 모델은 각 단어 쌍의 확률을 이전 단어의 등장에 조건화하여 계산합니다. 즉, 어떤 단어가 다른 특정 단어 다음에 올 확률을 모델링합니다. 이는 단어의 순서를 일정 부분 고려하게 되어, Uni-gram 모델보다는 문맥을 더 반영할 수 있습니다.

### **Tri-gram (3-gram)**

Tri-gram 모델은 텍스트를 세 단어의 연속으로 분석합니다. 이 모델은 두 단어의 시퀀스가 주어졌을 때 다음 단어의 등장 확률을 계산합니다. Tri-gram 모델은 Bi-gram 모델보다 더 많은 문맥 정보를 반영할 수 있지만, 모델의 복잡도와 필요한 데이터 양이 증가하는 단점도 있습니다.

### **N-gram 모델의 한계**

N-gram 모델은 간단하고 구현하기 쉽다는 장점이 있지만, 몇 가지 한계도 가집니다. 첫째, '희소성 문제(sparsity problem)'는 충분히 크고 다양한 데이터를 가지고 있지 않을 경우, 일부 N-gram의 등장 확률을 정확히 추정하기 어렵게 만듭니다. 둘째, '저장 공간 문제'는 N이 커질수록 고려해야 할 N-gram의 수가 기하급수적으로 증가하므로, 모델을 저장하고 처리하는 데 필요한 공간과 계산 리소스가 매우 커질 수 있습니다. 마지막으로, 이 모델은 고정된 크기의 문맥만을 고려할 수 있으며, 더 긴 의존성을 모델링하기 어렵습니다.

따라서 현대의 NLP 응용 프로그램에서는 N-gram 모델을 기본적인 특성 추출 방법으로 사용하거나, 보다 복잡한 딥러닝 기반의 모델과 결합하여 사용하는 경우가 많습니다.

## 언어모델 평가지표

언어 모델의 평가에 있어서 'Perplexity' (PPL)는 매우 중요한 지표 중 하나입니다. PPL은 언어 모델이 주어진 텍스트 데이터에 대해 얼마나 "당황"하지 않는지, 즉 얼마나 예측을 잘하는지를 수치적으로 나타내는 지표입니다. 이해를 돕기 위해 PPL과 다른 텍스트 표현 방식인 One-hot Encoding에 대해 설명하겠습니다.

### **Perplexity (PPL)**

- **정의**: Perplexity는 언어 모델이 특정 시퀀스를 얼마나 잘 예측하는지를 나타내는 지표입니다. 모델이 시퀀스의 각 단어를 예측할 때의 불확실성 또는 복잡성을 측정합니다.
- **계산**: 언어 모델의 PPL은 주어진 테스트 데이터셋에 대한 모델의 확률 분포를 기반으로 계산됩니다. 일반적으로, 모델이 데이터셋의 단어 시퀀스를 얼마나 잘 예측하는지에 대한 역확률의 기하 평균으로 정의됩니다.
- **해석**: PPL의 값이 낮을수록 모델의 성능이 좋다는 것을 의미합니다. 즉, 모델이 주어진 문맥에서 다음 단어를 예측하는 데 덜 당황하며, 더 확신을 가지고 예측한다는 것을 의미합니다.

### **One-hot Encoding**

- **정의**: One-hot Encoding은 범주형 변수를 처리할 때 사용되는 표현 방식입니다. 텍스트 데이터에서는 각 단어를 벡터로 표현할 때 사용됩니다. 이 방식에서는, 각 단어를 어휘 사전의 크기만큼의 길이를 가진 벡터로 변환하며, 해당 단어를 나타내는 인덱스의 위치만 1로 설정하고 나머지는 모두 0으로 설정합니다.
- **특징**: One-hot Encoding은 단어 간의 관계나 문맥을 전혀 고려하지 않습니다. 각 단어는 독립적인 벡터로 표현되며, 모든 단어 벡터는 동일한 거리를 가집니다. 이는 단어의 의미적 관계를 반영하지 못한다는 큰 단점을 가집니다.
- **용도**: 주로 모델의 입력 레이어에서 단어를 벡터로 변환하는 데 사용됩니다. 하지만 의미적 관계를 표현할 수 없다는 한계 때문에, 현대의 NLP 모델에서는 Word Embedding과 같은 다른 표현 방식이 더 자주 사용됩니다.

언어 모델 평가와 텍스트 표현 방식 모두 NLP 모델의 설계와 성능 향상에 있어 중요한 요소입니다. Perplexity는 모델이 얼마나 효과적으로 언어를 이해하고 예측할 수 있는지를 판단하는 데 도움을 주며, One-hot Encoding은 텍스트 데이터를 모델이 처리할 수 있는 형태로 변환하는 기본적인 방법 중 하나로 사용됩니다.

## RNN 특징

순환 신경망(Recurrent Neural Networks, RNN)은 자연어 처리(NLP)를 비롯해 시계열 데이터 처리에 널리 사용되는 신경망 구조 중 하나입니다. RNN의 주요 특징과 단점에 대해 좀 더 자세히 설명하겠습니다.

### **RNN의 특징**

1. **시퀀스 데이터 처리 능력**: RNN은 시퀀스 데이터를 처리하는 데 특화되어 있습니다. 각 시점(timestep)에서의 입력을 처리하고, 그 결과를 다음 시점으로 넘기는 구조를 가지고 있어서 시간에 따라 변화하는 데이터를 효과적으로 다룰 수 있습니다.
2. **내부 메모리를 가짐**: RNN은 이전 시점에서의 계산 결과를 내부 상태(state)로 저장하고 이를 다음 시점의 계산에 활용합니다. 이를 통해 이전 정보를 기억하면서 연속적인 데이터를 처리할 수 있는 능력을 갖추고 있습니다.
3. **변동적인 길이의 입력 처리 가능**: RNN은 입력 시퀀스의 길이가 고정되어 있지 않아도 처리할 수 있습니다. 이는 문장이나 문서와 같이 길이가 다양한 텍스트 데이터를 다룰 때 유용합니다.

### **RNN의 단점 및 문제점**

1. **베니싱 그레디언트 문제 (Vanishing Gradient Problem)**: RNN은 이론상으로는 긴 시퀀스에서의 의존성을 학습할 수 있지만, 실제로는 그레디언트가 사라지거나(explode) 또는 소멸(vanish)하는 문제로 인해 학습이 어렵습니다. 특히, 이전 정보가 너무 오래전 것이라면 그 영향력이 점점 약해져서 장기 의존성을 잘 학습하지 못하는 경향이 있습니다.
2. **그레디언트 폭발 문제 (Exploding Gradient Problem)**: 반대로 그레디언트가 너무 커져서 학습 과정에서 수치적으로 불안정해지는 현상도 발생할 수 있습니다. 이는 가중치가 급격히 커지게 하여 학습을 방해합니다.

### **해결 방안**

- **LSTM(Long Short-Term Memory)**: 베니싱 그레디언트 문제를 해결하기 위해 고안된 RNN의 한 변형으로, 장기 의존성을 효과적으로 학습할 수 있게 설계된 메모리 셀을 포함합니다.
- **GRU(Gated Recurrent Unit)**: LSTM을 간소화한 버전으로, 비슷한 성능을 유지하면서도 계산 비용을 줄일 수 있습니다.

RNN은 자연어 처리를 비롯한 다양한 분야에서 중요한 역할을 하지만, 위와 같은 단점들로 인해 복잡한 문제를 해결할 때는 LSTM, GRU와 같은 발전된 구조를 사용하는 것이 일반적입니다.

## 분포 가설과 분산표현

분포 가설과 분산 표현은 자연어 처리(NLP)에서 단어의 의미를 컴퓨터가 이해할 수 있는 형태로 모델링하는 핵심 개념입니다. 여기서 자세히 설명하겠습니다.

### **분포 가설 (Distribution Hypothesis)**

분포 가설은 "단어의 의미는 그 단어가 사용된 맥락(context)에 의해 결정된다"는 개념에 기반합니다. 이는, 단어가 비슷한 맥락에서 사용될수록 그 의미가 유사할 것이라는 가정을 포함하고 있습니다. 예를 들어, '은행'이라는 단어가 '저축', '대출', '금융'과 같은 단어들과 함께 등장한다면, 이는 '은행'의 의미가 이러한 맥락과 관련이 있다는 것을 나타냅니다.

### **분산 표현 (Distributed Representation)**

분산 표현 방식에서는 각 단어를 고정된 차원의 벡터로 표현합니다. 예를 들어, 128차원의 벡터로 모든 단어를 표현할 수 있습니다. 이러한 벡터 내의 각 차원이 특정한 의미를 가진다고 명시적으로 정의하지는 않습니다. 대신, 벡터 공간 내에서의 단어 벡터의 위치와 상대적인 거리가 단어 간 의미적 유사성을 반영합니다.

### **분산 표현의 장점**

1. **의미적 유사성**: 분산 표현을 사용하면, 벡터 간의 거리(예: 코사인 유사도)를 계산함으로써 단어 간 의미적 유사성을 측정할 수 있습니다. 이는 희소 표현(sparse representation)에서는 어려운 일입니다. 희소 표현에서는 단어를 독립적인 차원으로 처리하기 때문에, 단어 간의 유사성을 직접적으로 모델링하기 어렵습니다.
2. **차원의 저주 감소**: 고정된 차원의 벡터로 단어를 표현함으로써, 차원의 저주(curse of dimensionality) 문제를 완화할 수 있습니다. 단어의 개수에 비례하여 차원이 증가하는 희소 표현과 달리, 분산 표현에서는 모든 단어를 동일한 차원의 벡터로 표현합니다.
3. **맥락을 통한 학습**: 단어의 분산 표현은 주변 맥락을 통해 학습되므로, 단어가 사용되는 다양한 맥락을 효과적으로 포착할 수 있습니다. 이를 통해 모델은 단어의 다양한 의미와 사용 사례를 더 잘 이해할 수 있습니다.

Word2Vec, GloVe, FastText와 같은 단어 임베딩 방법들은 분산 표현을 생성하는 대표적인 알고리즘입니다. 이러한 알고리즘들은 대규모 텍스트 데이터로부터 단어의 의미를 벡터 공간에 효과적으로 임베딩하여, 단어 간 의미적 유사성을 반영하는 벡터를 학습합니다.

## One-hot encoding과 Embedding

텍스트 데이터를 수치적 형태로 변환하는 두 가지 다른 방식입니다. 이들 간의 주요 차이점을 이해하면 자연어 처리(NLP) 모델을 설계할 때 어떤 방식을 선택할지 결정하는 데 도움이 됩니다.

### **One-hot Encoding**

- **정의**: One-hot encoding은 각 단어를 어휘 사전(vocabulary)의 크기만큼의 벡터로 표현하는 방식입니다. 이 벡터에서 단어에 해당하는 위치만 1로 표시하고, 나머지는 모두 0으로 채웁니다.
- **특징**:
    - **희소성(Sparsity)**: 대부분의 값이 0이고 오직 하나의 위치만 1인 희소 벡터로 단어를 표현합니다.
    - **차원의 저주**: 어휘 사전의 크기가 크면 클수록 벡터의 차원도 커지며, 이는 계산 효율성과 모델의 성능을 저하시킬 수 있습니다.
    - **의미적 관계 부재**: 벡터 간의 수학적 연산이 의미적 유사성을 반영하지 못합니다. 모든 단어가 동등한 거리에 있으며, 단어 간 관계를 모델링하지 못합니다.

### **Embedding**

- **정의**: Embedding은 단어를 고정된 크기의 밀집 벡터(dense vector)로 표현하는 방식입니다. 각 차원은 연속적인 실수 값으로 구성되며, 단어의 의미적 특성을 모델링할 수 있습니다.
- **특징**:
    - **밀집성(Density)**: 모든 차원이 실수 값으로 채워진 밀집 벡터로 단어를 표현합니다. 이로 인해 벡터의 차원을 상대적으로 작게 유지할 수 있습니다.
    - **의미적 유사성**: 벡터 공간에서의 거리(예: 코사인 유사도)를 통해 단어 간 의미적 유사성을 반영할 수 있습니다. 유사한 의미를 가진 단어들은 벡터 공간에서 서로 가까이 위치하게 됩니다.
    - **계산 효율성**: 고정된 크기의 벡터를 사용하기 때문에, 어휘 사전의 크기에 관계없이 일관된 차원을 유지할 수 있으며, 계산 효율성이 높습니다.

### **결론**

One-hot encoding은 간단하고 이해하기 쉽지만, 고차원적이고 희소한 벡터로 인해 계산상 비효율적이며, 단어 간 의미적 관계를 포착하지 못한다는 단점이 있습니다. 반면, Embedding은 단어를 밀집 벡터로 표현하여 차원을 줄이고, 단어 간 의미적 유사성을 모델링할 수 있어, 현대의 NLP에서 널리 사용됩니다. Word2Vec, GloVe, FastText와 같은 알고리즘은 단어를 의미적으로 풍부한 벡터로 변환하여, 언어 모델링, 기계 번역, 감성 분석 등 다양한 NLP 작업에 기여합니다.

## Embedding 레이어

Embedding 레이어는 실제로 자연어 처리 모델에서 사용되는 "컴퓨터용 단어 사전"과 같은 역할을 합니다. 이 레이어는 단어를 대응하는 밀집 벡터로 변환하는 기능을 수행하며, 이러한 변환 과정을 "룩업 테이블(Lookup Table)"이라고 부릅니다. 여기서 더 자세히 설명해보겠습니다.

### **Embedding 레이어의 역할**

- **단어를 벡터로 변환**: Embedding 레이어는 각 단어를 고유한 실수 벡터로 매핑합니다. 이 벡터는 단어의 의미적, 문맥적 특성을 포착하는 데 사용됩니다.
- **차원 축소**: One-hot encoding과 달리, Embedding 레이어는 훨씬 낮은 차원에서 작동합니다. 예를 들어, 수만 개의 단어를 포함하는 어휘 사전을 사용하는 경우, one-hot encoding은 수만 차원의 벡터를 생성하지만, Embedding 레이어는 예를 들어 100~300차원 벡터로 이를 효율적으로 표현할 수 있습니다.
- **학습 가능**: Embedding 레이어의 벡터는 초기에 랜덤하게 설정되지만, 학습 과정을 통해 최적화됩니다. 이는 특정 작업(예: 문장 분류, 기계 번역)에 맞게 단어의 의미적 특성을 반영하도록 조정됩니다.

### **룩업 테이블 (Lookup Table)**

- **작동 방식**: 각 단어는 어휘 사전에서 고유한 인덱스를 가지며, Embedding 레이어는 이 인덱스를 사용하여 해당 단어의 벡터를 조회합니다. 이 과정이 바로 "룩업"입니다.
- **예시**: 어휘 사전이 {'the': 0, 'cat': 1, 'sat': 2}와 같이 정의되어 있고, 'cat'의 Embedding이 [0.25, -0.75]라고 할 때, 'cat'에 대한 인덱스 1을 Embedding 레이어에 입력하면, 레이어는 [0.25, -0.75]를 출력합니다.
- **효율성**: 이 방식은 매우 효율적입니다. One-hot 벡터를 사용하여 전체 어휘 사전을 검색하는 대신, 필요한 단어의 벡터만을 빠르게 추출할 수 있습니다.

### **결론**

Embedding 레이어는 자연어 처리 모델에서 중요한 구성 요소입니다. 이를 통해 모델은 단어의 의미적 특성을 효과적으로 학습하고, 이를 다양한 NLP 작업에 활용할 수 있습니다. 학습 가능한 파라미터를 통해 모델은 특정 작업에 최적화된 단어의 표현을 발전시킬 수 있으며, 이는 모델의 성능 향상에 기여합니다.

## One-hot encoding 장점 및 단점

One-hot encoding은 텍스트 데이터를 수치적으로 표현하는 가장 기본적인 방법 중 하나로, 주로 자연어 처리(NLP) 분야에서 사용됩니다. 이 방법의 장점과 단점을 살펴보겠습니다.

### **장점**

1. **간단하고 이해하기 쉬움**: One-hot encoding은 구현이 매우 간단하며, 각 단어가 어휘 사전에서 어떤 위치에 있는지 쉽게 이해할 수 있게 해줍니다.
2. **명확한 구분**: 모든 단어는 벡터 공간에서 서로 직교(orthogonal)합니다. 이는 각 단어가 고유하게 표현되며, 다른 단어와 명확히 구분된다는 것을 의미합니다.
3. **결정적인 표현**: 한 번 어휘 사전이 결정되면, 어떤 단어에 대해서도 일관된 방식으로 벡터를 생성할 수 있습니다. 이는 변환 과정에서의 불확실성을 제거합니다.

### **단점**

1. **차원의 저주**: 어휘 사전의 크기가 커질수록 one-hot 벡터의 차원도 비례해서 커집니다. 이는 대규모 어휘 사전을 가진 NLP 문제에서 매우 비효율적이며, 계산 비용과 메모리 사용량을 크게 증가시킵니다.
2. **희소성(Sparsity)**: 대부분의 요소가 0이고, 오직 한 위치만 1인 희소 벡터를 생성합니다. 이러한 희소성은 모델이 데이터에서 패턴을 학습하는 데 있어 비효율적일 수 있습니다.
3. **의미적 정보 부재**: One-hot 벡터는 단어 간의 의미적 유사성이나 관계를 전혀 포착하지 못합니다. 예를 들어, '왕'과 '군주'가 의미적으로 유사하더라도, 그들의 one-hot 벡터는 완전히 다르며, 이 사이의 유사성을 표현할 수 없습니다.
4. **확장성 문제**: 새로운 단어가 추가될 때마다 어휘 사전과 모든 one-hot 벡터를 업데이트해야 합니다. 이는 특히 지속적으로 데이터가 업데이트되는 시스템에서 관리하기 어려울 수 있습니다.

이러한 단점들로 인해, 많은 현대의 NLP 모델과 애플리케이션에서는 단어의 의미적 정보를 더 잘 포착하고, 계산적으로 효율적인 Embedding 방식을 선호합니다.

## word2vec – cbow

Word2Vec의 Continuous Bag of Words (CBOW) 모델은 자연어 처리에서 널리 사용되는 단어 임베딩 방법 중 하나입니다. CBOW 모델의 목표는 주변 단어(문맥)를 바탕으로 중심 단어를 예측하는 것입니다. 이를 통해 모델은 단어의 의미를 벡터 공간에 효과적으로 매핑할 수 있습니다. CBOW 모델의 핵심 개념을 설명하겠습니다.

### **중심 단어 (Target Word)**

- **정의**: 문장에서 예측해야 하는 단어입니다. CBOW 모델의 학습 과정에서는 주변 단어들을 입력으로 사용하여 이 중심 단어를 예측하려고 합니다.

### **주변 단어 (Context Words)**

- **정의**: 중심 단어를 예측하기 위해 사용되는 단어들입니다. 이 단어들은 중심 단어의 앞과 뒤에 위치하며, 모델이 중심 단어를 예측하는 데 필요한 문맥 정보를 제공합니다.

### **윈도우 (Window)**

- **정의**: 중심 단어를 예측하기 위해 고려하는 주변 단어의 범위입니다. 예를 들어, 윈도우 크기가 2라면, 중심 단어의 앞과 뒤에서 각각 2개의 단어를 주변 단어로 사용합니다.

### **슬라이딩 윈도우 (Sliding Window)**

- **정의**: 학습 과정에서 전체 텍스트를 통해 이동하면서 주변 단어와 중심 단어의 선택을 변경하는 기법입니다. 이 방식을 통해 모델은 다양한 문맥에서 각 단어의 사용을 학습할 수 있습니다. 슬라이딩 윈도우는 텍스트의 처음부터 끝까지 이동하며, 각 위치에서 윈도우에 포함된 주변 단어들을 바탕으로 중심 단어를 예측하려고 시도합니다.

CBOW 모델은 이러한 구조를 통해 주변 단어의 평균적인 정보(또는 합)를 사용하여 중심 단어를 예측합니다. 이 과정에서 학습된 단어 임베딩은 단어 간의 의미적 유사성을 반영하게 됩니다. 예를 들어, 비슷한 문맥에서 자주 사용되는 단어들은 벡터 공간에서 서로 가까워집니다. CBOW는 특히 단어의 발생 빈도가 높을 때 더 잘 작동하며, 전체 문맥을 고려한 단어의 의미를 효과적으로 포착할 수 있습니다.

## word2vec – skip-gram

Word2Vec의 Skip-gram 모델은 Continuous Bag of Words (CBOW) 모델과 반대의 작동 원리를 가지고 있습니다. Skip-gram 모델의 주요 목적은 주어진 중심 단어로부터 주변의 여러 단어들을 예측하는 것입니다. 이 방식은 단어의 임베딩을 학습함으로써, 단어 간의 의미적 관계를 효과적으로 포착할 수 있게 합니다. Skip-gram 모델의 핵심 개념을 설명하겠습니다.

### **중심 단어 (Input Word)**

- **정의**: Skip-gram 모델에서는 문장 내의 한 단어를 중심 단어로 선택하고, 이 중심 단어를 사용하여 주변의 여러 단어들을 예측합니다. 이 중심 단어가 모델의 입력으로 사용됩니다.

### **주변 단어 (Target Words)**

- **정의**: 중심 단어를 바탕으로 예측해야 하는 단어들입니다. Skip-gram 모델은 한 개의 중심 단어에서 시작하여 주변에 있는 하나 또는 여러 단어들을 예측하려고 시도합니다.

### **윈도우 (Window)**

- **정의**: 중심 단어로부터 주변 단어를 선택할 때 고려하는 범위입니다. 윈도우 크기가 주어지면, 중심 단어로부터 앞과 뒤로 몇 개의 단어를 주변 단어로 고려할지 결정합니다. 예를 들어, 윈도우 크기가 2라면 중심 단어의 앞과 뒤에서 각각 2개의 단어가 주변 단어로 선택됩니다.

### **슬라이딩 윈도우 (Sliding Window)**

- **정의**: 전체 텍스트를 통해 이동하면서 중심 단어와 주변 단어의 선택을 변경하는 과정입니다. 슬라이딩 윈도우를 사용함으로써 모델은 다양한 문맥에서 단어의 사용을 학습할 수 있습니다. 이 과정에서 중심 단어와 주변 단어 간의 관계를 반복적으로 학습하게 됩니다.

Skip-gram 모델은 특히 희소한 데이터 또는 드물게 등장하는 단어들에 대해 더 효과적인 성능을 보입니다. 각 중심 단어에 대해 여러 주변 단어들을 예측함으로써, 단어의 임베딩은 주변 단어들과의 의미적 관계를 반영하게 됩니다. 이 방식은 단어 간의 다양한 의미적 유사성을 포착하고, 벡터 공간에서 이를 표현하는 데 매우 유용합니다. Skip-gram 모델은 단어 임베딩을 생성하는 과정에서 단어의 다양한 문맥적 특성을 잘 반영할 수 있으며, 이를 통해 더 정교한 자연어 처리 작업을 수행할 수 있습니다.

입력층과 출력층에 대한 설명은 Continuous Bag of Words (CBOW) 모델의 구조를 설명하는 데 사용되었습니다. 그러나 제공된 설명은 CBOW와 Skip-gram 모델 둘 다에 적용될 수 있는 일부 요소들을 포함하고 있습니다. 여기서는 CBOW 모델의 관점에서 설명을 명확히 해보겠습니다.

### **CBOW 모델의 입력층과 출력층**

### 입력층 (Input Layer)

- CBOW 모델에서 입력층은 주변 단어들의 one-hot 벡터로 구성됩니다. 사용자가 정한 윈도우 크기에 따라, 중심 단어의 앞뒤로 위치한 주변 단어들이 선택됩니다.
- 예를 들어, 윈도우 크기가 2이고, 문장이 "The quick brown fox jumps over the lazy dog" 일 때, 중심 단어가 "fox"라면 주변 단어는 "quick", "brown", "jumps", "over"가 될 것입니다.
- 이 주변 단어들은 각각의 one-hot 벡터로 변환되어 입력층으로 전달됩니다. 모델은 이 입력들을 합치거나 평균내어 projection layer에 전달합니다.

### 프로젝션 층 (Projection Layer)

- 입력층에서 받은 주변 단어들의 벡터들을 합치거나 평균내어 얻은 벡터를 사용합니다. 이는 실제로는 임베딩을 학습하는 데 사용되는 가중치 행렬과의 곱셈으로 이루어집니다.
- 프로젝션 층은 단어의 one-hot 표현을 해당 단어의 임베딩으로 변환하는 역할을 합니다.

### 출력층 (Output Layer)

- 출력층의 목표는 예측하고자 하는 중간(중심) 단어입니다. 이 때, 중간 단어의 one-hot 벡터가 레이블(정답)로 사용됩니다.
- 출력층에서는 모델이 주변 단어들의 정보를 바탕으로 중심 단어의 one-hot 벡터를 예측하려고 합니다. 이 과정은 softmax 함수를 통해 전체 어휘 사전에 걸친 확률 분포를 생성하고, 이 분포를 통해 가장 확률이 높은 단어를 중심 단어로 예측합니다.

CBOW 모델은 이런 구조를 통해 주변 단어들의 문맥을 학습하고, 이를 바탕으로 중심 단어를 예측함으로써 단어의 의미적 임베딩을 생성합니다. 모델 학습 과정에서는 실제 중심 단어의 one-hot 벡터와 모델이 예측한 확률 분포 사이의 차이를 최소화하는 방향으로 가중치를 조정합니다.

목차

03 Word Representation

언어모델의 중요성

텍스트 표현하기

Embedding

Word2Vec

Glove

FastText

언어모델의 필요성

동계적 언어 모델(SLM)

단어열이 가지는 확률분포를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델

모델의 목표는 실제로 많이 사용하는 단어열(문장)의 분포를 정확하게 근사하는데 목표를 둠

자연어 분포를 근사할 정도의 충분한 말뭉치가 아니라면 언어를 정확히 모델링하지 못하는 문제점 발생

N-gram 언어모델

Uni-gram

Bi-gram

Tri-gram

언어모델 평가지표 : Perplexity

언어 모델을 평가하기 위한 내부 평가지표! 중여서 PPL이라 표현

PPL은 수치가 낮을수록 언어 모델의 성능이 좋다는 것을 의미

One-hot Encoding

RNN 특징

입력 후 결과를 다음 스테이트로 넘겨주는 과정이 필요

이전 스테이트의 기억을 가지고 다음으로 넘어감

단점! 이전 스테이트의 기억들이 과정을 거칠수록 점점 소멸됨

(베니싱 그레디언트)

분포 가설과 분산표현

모든 단어를 고정차원 (ex.128차원)의 벡터로 표현!

어떤 차원이 특정한 의미를 가진다고 정하지 않을 것

(단지 가정!) 유사한 맥락에서 나타나는 단어는 그 의미도 비슷할 것이다.

분포 가설(deistribution hypothesis)

맥락이라는 건 단순하게 단어 좌우에 출현하는 다른 단어들!

단어의 분산표현(Distribution Representation)

분산 표현을 사용하면, 희소 표현과 다르게 단어간 유사도를 계산할 수 있다.

One-hot vector와 Embedding 차이

Embedding Layer개념 이해하기

Embedding 레이어는 간단하게 말하면 컴퓨터용 단어 사전이다.

룩업 테이블(Lookup Table)

one-hor encoding

장점

단점

워드임베딩 (word Embedding)

Word2Vec

Glove

FastText

word2vec – cbow

중심단어 : 예측해야하는 단어

주변단어 : 예측에 사용되는 단어

윈도우 : 중심단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지 결정

슬라이딩 윈도우 : 주변 단어와 중심단어의 선택을 변경하면서 학습

입력층은 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 one-hot vector로 projection layer에 전달

출력층 : 예측하고자 하는 중간 단어의 one-hot vector가 레이블(정답)으로 필요

word2vec – skip-gram