
### **RNN의 한계**

**RNN의 한계** : 기존 RNN은 각 타임스텝에서 입력을 받고 즉시 출력을 내놓기 때문에, 입력 시퀀스의 전체 컨텍스트를 고려하지 못하는 경우가 많다. 계산 과정에서 정답 단어와 입력단어가 어떤 정답 단어로 학습되어야 하는지 제대로 학습이 이루어지지 않는다.

### Seq2Sqeq 해결방안

**Seq2Seq의 해결 방안**: 어떤 정답 단어로 학습되는지 제대로 학습이 이루어지지 않기 떄문에 정답단어에 대한 모델을 하나 더 만들어 변화를 주었다. 시퀀스 투 시퀀스 모델에서는 인코더와 디코더를 도입했다. 인코더는 입력 시퀀스를 고정된 크기의 벡터로 변환하여 이해하는 역할을 한다. 디코더는 이 인코더에서 나온 벡터를 사용하여 정답 시퀀스를 생성한다. 이렇게 함으로써, 인코더는 입력에 대한 표현을 학습하고, 디코더는 이 표현을 바탕으로 정답을 생성하는 구조를 가지게 되었다.

## Sequence to Sequence

개념 : sequence가 입력으로 들어가 sequence가 출력되는 구조

특징 : rnn의 고질적 문제인 long-term dependency 문제를 해결하기 위해 LSTM/GRU를 사용한다.

### **인코더-디코더 구조**

이러한 문제를 해결하기 위해, 인코더-디코더(Encoder-Decoder) 구조가 탄생. 이 구조에서는 두 개의 RNN을 사용한다: 하나는 입력 시퀀스를 처리하는 인코더, 다른 하나는 인코더로부터의 정보를 바탕으로 출력 시퀀스를 생성하는 디코더이다.

- **인코더**:
    - 인코더는 입력 시퀀스를 받아 처리하고, representation을 생성한다.
    - 입력 시퀀스를 고정된 크기로 받지 않고 각 단어의 정보를 하나의 고정된 사이즈의 벡터, context vector로 압축한다. 이 벡터는 입력 시퀀스의 전체적인 정보를 담고 있으며, 시퀀스의 길이에 관계없이 일정한 크기를 유지한다.
    - 예)
        - 입력시퀀스 : 저는 ai를 좋아합니다.
        - context vector : [0.5 ,0,8, 0.1]
- **디코더**:
    - **디코더**는 정답 단어를 생성하기 위해 인코더 representation을 사용한다.
    - 인코더가 생성한 콘텍스트 벡터를 사용하여 출력 시퀀스를 생성한다.
    - 출력 생성 과정은 보통 시작 토큰(SOS)으로 시작하여, 종료 토큰(EOS)이 등장할 때까지 계속된다.
    - 인코더로부터 받은 콘텍스트 벡터를 사용하여, 단계별로 출력 시퀀스를 생성하며 디코더는 이 콘텍스트 벡터를 기반으로 각 시점에서 어떤 출력을 생성할지 결정한다.

### 주요 구성요소 정리

- 인코더(Encoder): 입력 시퀀스를 받아 처리하고, 각 시점의 LSTM은 이전 상태의 정보와 현재 입력을 결합하여 처리한다. 이 과정은 임베딩 레이어(Embedding Layer)를 거쳐진 입력이 각 LSTM 셀을 통과하면서 진행된다. 인코더의 마지막 LSTM 셀에서 나온 출력은 컨텍스트 벡터(Context Vector)로, 전체 입력 시퀀스의 정보를 압축하여 포함한다.
- 디코더(Decoder): 컨텍스트 벡터를 받아 시작 토큰 SOS(Start of Sequence)로 시작하여 타깃 시퀀스를 생성하기 시작한다. 각 LSTM 셀은 이전 상태와 현재 입력(이전에 생성된 토큰)을 기반으로 다음 토큰을 예측한다. 이 예측은 소프트맥스(Softmax) 레이어를 통해 단어장의 단어들에 대한 확률 분포로 변환된다. 그리고 가장 높은 확률을 가진 단어가 다음 토큰으로 선택된다.
- 임베딩 레이어(Embedding Layer): 정수 인코딩된 토큰들을 고차원의 벡터로 변환해준다. 이 벡터들은 LSTM이 처리할 수 있는 형태로, 단어의 의미적인 정보를 포함하도록 학습된다.
- 소프트맥스(Softmax): 디코더의 각 스텝에서, LSTM의 출력을 단어장의 크기와 같은 벡터로 매핑하고, 소프트맥스 함수를 사용해 해당 벡터를 확률 분포로 변환한다. 이 확률 분포는 다음 단어를 예측하는 데 사용된다.
- 특별 토큰(Special Tokens): SOS는 시퀀스의 시작을, EOS(End of Sequence)는 시퀀스의 끝을 나타낸다. 이 토큰들은 디코더가 언제 문장 생성을 시작하고 끝내야 하는지 알 수 있게 한다.

### seq2seq의 과정

1. 인코더의 임베딩 레이어에서 입력 시퀀스가 정수 인덱스로 변환된 후 고차원의 벡터로 임베딩된다.
2. 2. 임베딩된 벡터는 LSTM 셀을 거치면서 처리되고, 각 LSTM 셀은 이전 상태의 정보를 현재 입력과 함께 사용한다.
3. 인코더의 마지막 LSTM 셀에서 나온 출력은 컨텍스트 벡터로 압축된다. 이 벡터는 입력 시퀀스의 전체 정보를 포함한다.
4. 디코더는 컨텍스트 벡터와 시작 토큰 SOS를 받아 첫 번째 토큰을 생성하기 시작한다.
5. 디코더의 LSTM 셀은 컨텍스트 벡터, 이전 상태, 이전 토큰(또는 예측된 토큰)을 사용해 다음 토큰을 예측한다.
6. 예측된 각 토큰은 소프트맥스 레이어를 통과하여 단어장에 있는 각 단어에 대한 확률 분포로 변환된다.
7. 소프트맥스 레이어에서 가장 높은 확률을 가진 단어가 선택되어 다음 토큰으로 출력된다.
8. 8. 이 과정은 EOS 토큰이 생성될 때까지 반복되며, EOS는 문장의 끝을 나타낸다.

## Seq2Seq의 문제점

**문제점 :**

기존 RNN의 장기 의존성 문제를 어느 정도 해결하지만, 여전히 긴 시퀀스를 처리할 때 정보를 손실할 가능성이 있다.

그레디언트 소실 문제(Vanishing Gradient Problem)와 같은 문제는 LSTM을 사용함으로써 부분적으로 해결될 수 있지만, 여전히 멀리 있는 정보는 잊혀져 가고 하나의 고정된 크기의 벡터에 모든 정보를 압축하려다보니 정보 손실이 발생한다. 앞쪽 단어 비중이 유실되고 입력 시퀀스의 끝 부분에 더 많은 중요도를 부여하게 되므로, 이를 해결하기 위해 다양한 시도를 했다.

**⇒ bottleneck problem**(인코더에서 생성된 고정된 크기의 컨텍스트 벡터가 모든 입력 정보를 담아내야 한다는 점에서 오는 한계를 지칭)

**해결책 :**

- 시도) 첫 단어부터 잘 맞추게 하자는 목적으로 어순을 거꾸로 배열하는 시도 ⇒ but 여전히 인코더 문장의 모든 정보를 담아내지 못해서⇒ 해결 실패.
- 시도) 중요한 단어에 집중하여 디코더에 바로 전달하기로 함⇒ attention 매커니즘

**attention의 등장:**

어텐션 메커니즘(Attention Mechanism)을 도입하여 중요한 정보에 더 많은 초점을 맞추고, 모델이 전체 입력 시퀀스에서 각 시점마다 필요한 정보를 동적으로 추출할 수 있게 했다.

## Attention

- 디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서의 전체 입력 문장을 한번 더 참고함
- 전체 입력 문장을 동일한 비율로 참고하는 것이 아니라 해당 시점에서 예측해야되는 단어와 연관성이 있는 입력 단어에 좀더 집중함.

어텐션은 모델이 필요한 정보에 집중할 수 있도록 하여, 이 문제를 효과적으로 해결한다. 어텐션은 각 디코더의 스텝에서 인코더의 전체 출력 시퀀스에 대해 가중치를 계산하여, 해당 스텝에서 필요한 정보를 동적으로 선택한다. 이렇게 함으로써, 모델은 입력 시퀀스의 어떤 부분이 현재 생성하고 있는 단어와 더 관련이 있는지를 학습하고, 그 정보에 더 많은 비중을 두게 된다.

이는 시퀀스의 끝 부분뿐만 아니라, 전체 시퀀스에 걸쳐 중요한 정보를 보존하는 데 도움을 주어 번역의 질을 향상시킨다.

### Attention 과정

1. 인코더는 입력 시퀀스의 각 단어(또는 토큰)를 숨겨진 상태(h)로 변환한다. 예: h1(저는), h2(ai를), h3(좋아합니다).
2. 디코더는 각 시점(s)에서 이전 단어를 기반으로 다음 단어를 예측하려고 한다. 예: s1(SOS)에서 시작하여 s2(I), s3(like), s4(AI)를 순차적으로 예측.
3. 디코더의 각 시점에서, 인코더의 모든 숨겨진 상태와의 유사도(어텐션 스코어)를 계산하기 위해 내적(dot product)을 사용한다. 이는 '쿼리(query)'와 '키(key)' 벡터를 비교함으로써 수행된다.
4. 계산된 어텐션 스코어는 소프트맥스 함수를 통해 확률(알파)로 변환되며, 이는 각 인코더 상태의 중요도를 나타낸다.
5. 각 인코더 상태의 가중치를 적용하여(어텐션 가중치와 인코더의 상태를 곱하여), 최종 어텐션 값(가중합)을 계산한다.
6. 이 어텐션 값은 디코더의 현재 상태와 결합되어, 리니어 레이어와 소프트맥스 함수를 통해 다음 단어를 예측하는 데 사용된다.
7. 이 모든 과정은 디코더가 EOS 토큰을 생성할 때까지 반복된다.

어텐션 스코어를 구하는 방식은 여러 가지가 있으며, 가장 단순한 형태는 내적을 사용하는 'dot-product'이다. 다른 방법으로는 'Bilineart'이나 다층 퍼셉트론(multi-layer perceptron)을 사용하는 방법이 있다.

어텐션 메커니즘이 모델에 추가되면, 역전파(backpropagation) 시에 어텐션을 통한 '지름길'이 생기며, 이는 모델이 각 단어의 중요도를 더 잘 학습하도록 하고, 그레디언트 소실 문제를 더 완화시킨다.

마지막으로, 어텐션 메커니즘은 모델이 각 단어에 얼마나 주목했는지 시각화하여, 모델의 해석 가능성(interpretability)을 향상시킨다. 이를 통해, 모델이 번역이나 요약 작업을 할 때 어떤 단어가 결정적인 역할을 했는지 확인할 수 있다.

```python
어텐션 스코어를 계산하는 구체적인 단계:
디코더의 쿼리 벡터(Query Vector): 디코더의 현재 스텝의 숨겨진 상태로, 다음 단어를 예측하는 데 필요한 '질문'의 역할을 한다.

인코더의 키 벡터(Key Vector): 인코더의 각 스텝에서 출력되는 숨겨진 상태로, 입력 시퀀스의 각 단어/토큰에 대한 정보를 담고 있다.

쿼리 벡터는 인코더의 모든 키 벡터와 비교된다. 이 '비교'는 내적(dot product)을 통해 수행된다. 즉, 디코더의 쿼리 벡터와 인코더의 각 키 벡터 간의 내적을 계산한다.

내적의 결과로 나온 값은 어텐션 스코어로 사용되며, 이는 디코더의 현재 스텝에서 인코더의 어떤 출력이 다음 단어와 가장 관련이 높은지 나타낸다. 높은 어텐션 스코어는 높은 관련성을 의미한다.
계산된 어텐션 스코어는 소프트맥스 함수를 통해 확률(어텐션 가중치)로 변환된다.
```

### Attenttion 요약

- 어텐션 메커니즘은 기계 번역 등 seq2seq 모델의 한계를 극복하기 위해 개발되었다. 기존의 seq2seq 모델은 입력 시퀀스를 고정된 크기의 벡터로 변환하여 정보를 전달했는데, 시퀀스가 길어질수록(time-step이 멀어질수록) 정보 손실로 인해 번역 품질이 떨어지는 문제가 있었다. 이를 bottleneck problem(병목현상)이나 vanishing gradient problem(기울기 소실 문제)라고 부른다.
- Attention이 문제를 어느정도 해결한다. 디코더의 각 time-step에서 전체 인코더의 출력에 접근할 수 있도록 하여, 중요한 정보를 선택하여 집중하는 능력을 부여한다. 원하는 단어로 gradient를 전달할 수 있는 경로(지름길)이 존재하여 멀리 있는 단어여도 gardient를 손실과 변형 없이 전달 가능하도록 하였다.
- 이로써, 모든 입력 정보가 동일한 중요도를 갖고 디코더의 각 단계에 영향을 미치게 되므로, 어텐션을 사용하지 않는 LSTM 방식보다 긴 시퀀스를 처리할 때 정보를 더 잘 보존한다. 또한, 많은 time-step을 지나치는 과정을 거치지 않아도 되어졌다.
- Attention의 패턴을 조사하면 Decoder가 각 단어 예측했을 때 어떤 Encoder에 집중하였는지 분석이 가능하여 모델 해석력을 향상시킨다.

요약하자면, 어텐션 메커니즘은 seq2seq 모델의 성능을 향상시키는 기술로, 디코더가 생성할 다음 단어를 예측할 때 인코더의 모든 시점에서 출력된 정보를 활용하여 더 맥락에 맞는 번역이나 문장 생성이 가능하게 만든다.