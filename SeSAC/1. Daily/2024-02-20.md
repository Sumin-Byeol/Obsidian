 
### LSTM

LSTM은 장기 의존성 문제를 해결하기 위해 cell state와 복잡한 게이트 메커니즘을 사용하는 RNN의 한 형태이다

1. **데이터 입력**: 시퀀스의 각 타임스텝에서 데이터 포인트(예: 텍스트에서의 단어 또는 시계열 데이터에서의 시점)가 LSTM 네트워크에 입력된다.
2. **Forget Gate**:
    - 이 게이트는 현재 입력 **`x_t`**와 이전 타임스텝의 hidden state **`h_(t-1)`**을 받아들인다.
    - 이 데이터는 가중치 행렬과 결합하고 bias가 더해진 후 시그모이드 함수를 통과하여 0과 1 사이의 값을 출력한다.
    - 이 출력값은 이전 셀 상태 **`C_(t-1)`**에 곱해져서 어떤 정보를 "잊어야 할지"를 결정한다.
3. **Input Gate와 후보 셀 상태 생성**:
    - 동시에 현재 입력과 이전 hidden state가 다른 가중치 행렬과 결합하고, 시그모이드 함수와 tanh 함수를 각각 통과하여 두 가지 다른 결과를 생성한다.
    - 시그모이드 함수 결과는 어떤 새로운 정보를 셀 상태에 추가할 것인지를 결정한다.
    - tanh 함수 결과는 새로운 후보값 **`C̃_t`**을 생성하여 가능한 새로운 셀 상태를 제시한다.
4. **셀 상태 업데이트**:
    - Forget Gate에서 생성된 결과와 새로운 후보값이 결합되어 현재 셀 상태 **`C_t`**를 업데이트한다.
    - 이전 셀 상태에 forget gate의 출력을 곱한 값에, input gate의 출력과 새로운 후보값의 곱을 더해 새로운 셀 상태를 형성한다.
5. **Output Gate와 hidden state 생성**:
    - 현재 입력과 이전 hidden state는 또 다른 가중치 행렬을 통과하고, 시그모이드 함수를 통과하여 output gate의 출력을 생성한다.
    - 업데이트된 셀 상태는 tanh 함수를 통과하고, 이 결과는 output gate의 출력과 곱해져 최종 hidden state **`h_t`**를 형성한다.
6. **출력**:
    - 생성된 hidden state **`h_t`**는 다음 타임스텝의 LSTM 셀로 전달될 뿐만 아니라, 필요에 따라 출력층으로 전달되어 예측이나 결정을 생성한다.

이 과정을 통해 LSTM은 각 타임스텝에서 중요한 정보를 유지하고 불필요한 정보를 버리면서, 시퀀스의 전체 길이에 걸쳐 학습과 기억을 수행한다.

```python
Forget Gate: 이전 셀 상태에서 불필요한 정보를 제거한다. 수식은 f_t = σ(W_f · [h_(t-1), x_t] + b_f)이며, 여기서 σ는 시그모이드 활성화 함수다.

Input Gate: 새로운 정보를 셀 상태에 추가하는 역할을 한다. 수식은 i_t = σ(W_i · [h_(t-1), x_t] + b_i)와 C̃_t = tanh(W_C · [h_(t-1), x_t] + b_C)를 통해 새로운 후보값을 생성한다.

Cell State: 셀 상태는 네트워크를 통해 장기적인 정보를 전달한다. 업데이트는 C_t = f_t * C_(t-1) + i_t * C̃_t 수식으로 이루어진다.

Output Gate: 다음 hidden state로 어떤 값을 출력할지 결정한다. 수식은 o_t = σ(W_o · [h_(t-1), x_t] + b_o)와 h_t = o_t * tanh(C_t)로 계산된다.
```

### **GRU 구조:**

GRU(Gated Recurrent Unit)는 LSTM의 간소화된 버전으로, 비교적 적은 수의 파라미터를 가지고 있음에도 LSTM과 유사한 성능을 제공한다.

1. **Update Gate (업데이트 게이트)**:
    - 현재 입력 **`x_t`**와 이전 상태 **`h_(t-1)`**를 결합하여 현재 상태를 얼마나 업데이트할지 결정한다.
    - 수식: **`z_t = σ(W_z · [h_(t-1), x_t])`** 여기서 σ는 시그모이드 함수다.
    - 이 게이트는 새로운 정보를 어느 정도 받아들일지, 과거 정보를 얼마나 유지할지 결정한다.
2. **Reset Gate (리셋 게이트)**:
    - 이전 상태의 어떤 부분을 잊을지 결정하여 새로운 입력과의 결합에 영향을 준다.
    - 수식: **`r_t = σ(W_r · [h_(t-1), x_t])`**
    - 새로운 입력에 의해 현재 상태가 얼마나 재설정될지를 결정한다.
3. **Memory Candidate (메모리 후보)**:
    - 임시 메모리 상태인 새로운 정보를 생성한다.
    - 수식: **`h̃_t = tanh(W · [r_t * h_(t-1), x_t])`**
    - 리셋 게이트의 출력과 이전 상태의 상호작용을 통해 새로운 후보 상태를 계산한다.
4. **Final Memory (최종 메모리)**:
    - 업데이트 게이트와 메모리 후보를 결합하여 최종 상태를 결정한다.
    - 수식: **`h_t = (1 - z_t) * h_(t-1) + z_t * h̃_t`**
    - 업데이트 게이트가 결정한 비율에 따라 이전 상태와 새로운 후보 상태를 결합한다.

### **LSTM과 GRU의 차이점:**

- LSTM은 Forget Gate, Input Gate, Output Gate 및 cell state를 가지고 있다.
- 반면 GRU는 Update Gate와 Reset Gate를 가지며, LSTM의 cell state와 hidden state를 통합한 하나의 hidden state만을 가진다.
- GRU는 LSTM보다 파라미터 수가 적어서 계산 효율성이 더 높고, 더 간단하게 구현할 수 있다는 장점이 있다.

GRU는 특히 파라미터 수가 제한적인 경우나 빠른 학습이 필요할 때 유리하며, 짧은 시퀀스 데이터에 대해 LSTM과 유사한 성능을 낼 수 있다. 하지만 시퀀스 길이가 매우 길거나 복잡한 문제에 대해서는 LSTM이 더 우수한 성능을 보일 수 있다.