
# 07 RNN(Recurrent Neural Network)

## 시계열 데이터란?

### 시계열 데이터

- 시간 별로 구성된 값의 집합이다.
- 음성, 자연어 , 센서 데이터, 주가 등
- 기록 추세, 실시간 경고, 예측 모델링 분석

### 시계열 데이터는 무조건 RNN을 써야하는지

- **그렇지 않다**. ⇒ MLP, CNN이 성능이 더 좋을 때도 있다.
- 그럼에도 RNN을 권장하는 이유 ⇒ MLP와 CNN은 처음 한번만 입력받고 그 이후의 연산 과정에서 데이터를 추가적으로 고려하지 않는다. 이전의 연산결과가 현재 입력데이터가 함께 고려해야 하는 경우에 RNN을 권장한다.

## RNN 구조

- 상태를 계산할 때 이전 상태(state)를 사용하는 루프를 가지고 있다. 이전 시점의 상태를 현재 시점으로 전달하는 역할을 하며, 이를 통해 시간에 걸쳐 정보를 기억할 수 있다.
    
- RNN의 핵심은 '상태' 또는 '은닉 상태(hidden state)'라고 불리는 내부 변수이다. 이 상태는 RNN이 각 시점에서 입력과 함께 받아들여, 새로운 상태를 생성하는 데 사용된다.
    
- 각 시점(timestep)에서 RNN은 현재의 입력(**`xt`**)과 이전 시점의 상태(**`ht-1`**)를 받아서 현재 시점의 상태(**`ht`**)를 계산한다. 이 과정에서 **`ht`**는 **`xt`**의 정보와 **`ht-1`**의 정보가 결합된 형태로 업데이트 된다.
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/f9dfe933-8c19-4ebf-96e7-c5880594fc07/Untitled.png)
    

### RNN의 동작 원리

1. **현재 입력(`xt`)**: 각 시점에서 RNN은 외부에서 새로운 입력을 받는다. 이 입력은 현재 시점에서 처리할 데이터 포인트를 나타낸다.
2. **이전 상태(`ht-1`)**: 이전 시점의 상태는 이미 처리된 정보를 담고 있고, 이를 통해 RNN은 이전에 발생한 이벤트나 패턴에 대한 '기억'을 유지한다.
3. **현재 상태 계산(`ht`)**: 현재 입력과 이전 상태는 가중치와 결합되고, 이 결합된 정보는 일반적으로 비선형 활성화 함수(예: 하이퍼볼릭 탄젠트, ReLU 등)를 통과하여 현재 상태를 생성한다. 이 현재 상태는 다음 두 가지 주요한 작업을 위해 사용된다:
    - 다음 시점의 상태를 계산하는 데 사용됨
    - 현재 시점의 출력(예: 분류 레이블, 다음 시퀀스의 요소 등)을 생성하는 데 사용됨

### 수식으로 살펴보기

$$ h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h) $$

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/5ac2ee5d-a357-4561-85bc-2f425c9c0933/Untitled.png)

- GPT..
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/5713b5db-789b-4a76-8b05-9565972f91dc/Untitled.png)
    

### RNN 코드로 살펴보기

→ 정리예정

### RNN 기본구조 도식화

→ 정리예정

### RNN 학습과정

→ 정리예정

## 다양한 RNN 구조들

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/457fd5b8-cccc-4c58-8120-00b810198b94/8b5a0e84-cb97-4b7a-8be5-b9ca879bbdd0/Untitled.png)

### 1) One-to-Many

### 2) Many-to-One

### 3) Many-to-Many

## RNN의 한계점

### 기울기 소실 문제(Vanishing Gradient Problem)

RNN에서는 시간이 흐를수록 **처음의 입력이나 이전 상태의 영향력이 줄어들어, 기울기 소실 문제**가 있다.

- What - time - is - it - ? 가 있다면
- ⇒시간의 흐름에 따라 첫번째 입력인 ‘What’의 영향력이 점점 줄어들음
- ⇒ LSTM, GRU, ReLU로 해결

## LSTM, GRU

이런 문제를 해결하기 위해 LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit) 같은 변형 모델이 등장했다. 이들 모델은 기울기 소실 문제를 해결하기 위해 내부적으로 게이트 메커니즘을 사용한다.

- LSTM은 input gate, forget gate, output gate를 포함하여 cell state를 관리하는 메커니즘을 도입하여 기울기 소실 문제에 대응한다
- GRU는 이러한 게이트들을 업데이트 게이트와 리셋 게이트로 간소화하여 비슷한 문제를 해결한다.(LSTM의 중복된 연산을 제거)